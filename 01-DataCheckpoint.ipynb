{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Bob Barker:  Analysis, Software, Visualization\n",
    "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
    "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft\n",
    "\n",
    "Ja-Chan Lu: Updated research question, updated data after commentary on data section from project proposal.\n",
    "\n",
    "Tianlin Situ:\n",
    "\n",
    "Julian Luan:\n",
    "\n",
    "Tony Zhang:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates to Research Question:\n",
    "\n",
    "**Defining \"Eras\":**\n",
    "\n",
    "***Era 1 (1990-2000 seasons):*** Big-man dominated era where most points were scored within close distances to the basket as opposed to three-pointers (much further away from the basket).\n",
    "\n",
    "***Era 2 (2001-2013 seasons):*** League begins to shy away from traditional post-up and big-men due to the removal of the \"illegal defense\" which demanded players must be glued to the player they are defending at all times. With the abolishment of that rule, teams were allowed to play zone defense, which allowed players to occupy space on the court and prevent the other team from taking it instead of solely sticking to the player. In zone defenses, teams could theoretically just sufforcate the paint with their players and prevent anyone from making their way into the basket, taking away points from closer to the basket. To counter this, teams began to space out their players in the court, in attempts to work with the little space they have in the perimters to score from further away the basket. \n",
    "\n",
    "***Era 3 (2014-Present seasons):*** Teams analyzed shot attempts throughout their seasons and come to the realization that mid-range shots just are not as efficient as shooting 3-pointers. The chances of missing a mid-range and a three-pointer are the same, but three-pointers reward more points than a mid-range, meaning teams would much rather shoot 3's going forward. Teams now push all their players to attempt shooting 3-pointers, rather than mid-range jumpers, further increasing the volume of 3-pointers in the league.\n",
    "\n",
    "**Editting Reserach Question:**\n",
    "\n",
    "***New Research Question:*** How has the correlation between height and 3-point attempts changed across select NBA eras? Furthermore, we can then pursue another question: Is the increase in 3-point shooting volume only apply to taller positions (power forwards and centers) or does this affect all positions equally?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update to Data Proposal Section\n",
    "\n",
    "Actual Dataset Description:\n",
    "Our dataset will be from swar/nba_api python package, which is an open-source client that gives access to the official data API's on NBA.com. The package is widely used and reliable for accessing NBA data, as seen with their 3,400 GitHub stars. \n",
    "\n",
    "As mentioned earlier, the data comes directly from NBA.com's database that gives access to player statistics. For this project, we will utilize the playercareerstats endpoint which keeps records of the player information needed, starting from the 1990 NBA season up until the present day.\n",
    "\n",
    "For our analysis, we will sample 3-4 datasets (seasons) from each era defined above (1990-2000, 2001-2013, and 2014-present day. Under the assumption that each NBA season has around 500 players, we will have roughly 6000 total observations, where each observation contains a player's id, team_id, height, wingspan, position, points, three_point_attempts, games_played, and minutes_played.\n",
    "\n",
    "The data in the nba_api originates from official NBA box scores and logs, that were maintained by the league since the NBA was created. During 1990s-2000 era, the statistics were recorded by officials manually whereas 2001-present day, the data was logged and recorded by automated system. Whether it was automated or manually recorded, the data is still reliable and truthful as the NBA continues to verify recorded statistics both during and after games.\n",
    "\n",
    "Limitations to the data:\n",
    "1. Records of heights: Throughout all eras, players are to self-report height, and oftentimes may be rounded up/down, rather than 100% truthful.\n",
    "2. Position Labels: Although all players are given one specific position on paper, their roles on the court often change situationally, meaning no player is truly confined to one role throughout the whole season.\n",
    "3. In order to ensure accuracy, players who have less than 41 games_played or 10 minutes_played will not be considered in the dataset to prevent skewing the analysis as they will most likely have exceptionally high/low statistics that do not truly reflect their abilities as a player. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATED Ethics Section**\n",
    "## Ethics \n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "Our project uses publicly available NBA player statistics and biographical attributes (e.g., height, position) from official/public sports data sources accessed via `nba_api`. We are not collecting new data from human subjects, conducting interventions, or gathering private information. Because this is archival, public, non-sensitive data about public figures in a professional context, informed consent in the traditional research sense is not applicable.\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "\n",
    "Because we rely on an existing dataset, we inherit any biases from how the league records data and how positions/heights are defined. For example, “position” labels can be inconsistent across seasons and may reflect team conventions rather than actual on-court roles. We will mitigate this by (1) documenting data sources and definitions, (2) standardizing positions into broad groups (e.g., Guard/Wing/Big or G/F/C), and (3) running sensitivity checks with alternative groupings where feasible.\n",
    "\n",
    " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    "\n",
    "Although player names are technically identifying, they are public figures and names are not necessary for our analysis. We will minimize exposure by analyzing at the player-season level using player IDs and excluding unnecessary personal fields (birthdate, birthplace, etc.). If we include example players to illustrate outliers, we will keep it minimal and directly tied to analysis.\n",
    "\n",
    " - [X] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "We are not building a system that makes decisions about individuals, and we do not use protected attributes such as race, ethnicity, or gender. As a result, typical “downstream bias” concerns are limited. However, we will avoid making normative claims about what different groups “should” do and will frame findings as descriptive patterns in professional basketball strategy and roles.\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "\n",
    "We will store only public, non-sensitive statistical data in our course GitHub repository and local course environment. We will not store secrets (tokens) in the repo. If authentication tokens are required (e.g., GitHub PAT), they will be kept outside version control (environment variables or local credential storage).\n",
    "\n",
    " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    "\n",
    "Because the data are public professional records and we are not maintaining a user-facing database, this is not directly applicable. If needed, we can remove player names or omit individual examples from outputs and present aggregated results.\n",
    "\n",
    " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "We will retain only what is required to reproduce the analysis for the duration of the course. After the course, we can delete local copies and/or make the repository private/archived depending on team preference and course guidance.\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "\n",
    "Our analysis concerns professional sports strategy rather than impacted communities. Still, we will check assumptions using domain knowledge (e.g., rule changes, era differences, and the rise of “spacing” offenses) and avoid over-generalizing from basketball to broader claims about bodies or ability.\n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "\n",
    "Key confounds include era-wide increases in 3PA, changing offensive schemes, minutes played, and role differences within positions. To reduce omitted-variable bias, we will use rate-based outcomes (e.g., 3PA per 36 minutes or 3PA/FGA) and include controls such as minutes/games (and age if available). We will also test interactions with era and position to avoid attributing league-wide trends solely to height.\n",
    "\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    "\n",
    "We will avoid misleading axes/scales, show uncertainty where appropriate (confidence intervals), and present both overall trends and stratified trends by position/era. We will report effect sizes in interpretable units (e.g., change in 3PA rate per 2 inches) instead of relying only on p-values.\n",
    "\n",
    " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    "\n",
    "We will not publish unnecessary personal fields. Player names are not required for the core analysis and will be omitted from most tables/plots. If naming an outlier player is helpful, we will do so sparingly and only when it materially improves interpretation.\n",
    "\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "\n",
    "We will keep a reproducible pipeline: data pull script/notebook, cleaning steps with clear comments, fixed season lists/era bins, and saved intermediate datasets. We will include a README describing how to rerun the project from raw pull to final figures.\n",
    "\n",
    "### D. Modeling\n",
    " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    "\n",
    "Our models are descriptive and use basketball-related features (height, position, era, minutes/games). We will not include protected attributes. We will also avoid using height to draw claims about people outside the context of NBA roles and strategy.\n",
    "\n",
    " - [X] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    "\n",
    "We are not deploying a predictive model for decisions about people, so formal fairness testing (disparate error rates) is not directly applicable. Our primary goal is explanation/association. If we do include predictive components, we will report performance separately by position/era to ensure results are not dominated by one group.\n",
    "\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    "\n",
    "We will compare multiple reasonable outcomes (3PA/game, 3PA/36, 3PA/FGA) because “attempts” can be driven by playing time and team context. Using multiple metrics reduces the risk of drawing conclusions that are an artifact of one specific definition.\n",
    "\n",
    " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    "\n",
    "We will use interpretable models (correlations and linear regression with interaction terms) and will translate coefficients into plain language. For example, we will explain how the height–3PA relationship changes across eras and positions rather than presenting only statistical output.\n",
    "\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "\n",
    "We will explicitly state that correlation does not imply causation, and that position labels and evolving play styles confound simple interpretations. We will discuss how era-wide strategy changes (not height alone) drive modern 3PA, and we will caution against generalizing results beyond NBA context.\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    "\n",
    "This project is not being deployed as a real-world system. If we were to extend it, “monitoring” would mean updating with new seasons and checking whether the height–3PA relationship shifts over time (concept drift), which we partially address by explicitly modeling era effects.\n",
    "\n",
    " - [X] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    "\n",
    "Potential harm is low because this is descriptive sports analytics, but misinterpretation could occur (e.g., implying taller players “shouldn’t” shoot threes). We will reduce this risk through careful framing, clear limitations, and avoiding normative claims. If issues are identified, we can revise wording, remove unnecessary identification of individuals, and add clarifying caveats.\n",
    "\n",
    " - [X] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    "\n",
    "Not applicable because we are not deploying a production system. Our work exists as a report/notebook; if needed we can remove or revise analyses and update the repository.\n",
    "\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
    "\n",
    "Because we are not deploying, risk is limited. The main unintended use would be over-generalizing results to non-NBA contexts or using findings to support stereotypes about body types. We will prevent this by keeping claims strictly within basketball strategy/role context and explicitly stating boundaries of interpretation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATED Timeline Section**\n",
    "| Meeting Date | Meeting Time    | Agenda / Discuss at Meeting                                                                                                                                                                                                                            | Next Steps                                                                                                                              |\n",
    "| ------------ | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **2/16**     | 6 PM            | Finalize scope: choose eras/seasons (e.g., 90s/00s/10s or specific season ranges); decide unit of analysis (player-season); decide primary metric (3PA/game vs 3PA/36 vs 3PA rate = 3PA/FGA); confirm position grouping plan (G/F/C or Guard/Wing/Big) | Assign roles (data pull, cleaning, EDA, modeling, writing); set repo structure; draft data dictionary + analysis plan outline           |\n",
    "| **2/18**     | 6 PM            | Data Checkpoint work session: pull data via `nba_api`; verify joins (player_id ↔ season stats ↔ height); check coverage across eras; start cleaning (height standardization, position parsing, era labeling)                                       | Submit Data Checkpoint; export raw dataset v1 + cleaned dataset v1; document endpoints + code workflow in README                        |\n",
    "| **2/23**     | 6 PM            | Data cleaning review: handle missing height/position; decide filters (min minutes/games); engineer variables (3PA/36, 3PA rate); sanity checks by era/position (counts, ranges)                                                                        | Lock cleaning rules; produce final analysis-ready dataset; write short “Data & Cleaning” methods paragraph                              |\n",
    "| **2/25**     | 6 PM            | EDA sprint: scatter/hex plots of height vs 3PA metrics; stratify by era and position; check whether correlation differs by group; identify outliers (very tall high-3PA) and decide treatment                                                          | Finalize EDA notebook + 3–5 key figures; draft EDA narrative answering “overall vs by era/position”                                     |\n",
    "| **3/01**     | 6 PM            | Modeling plan + first results: run correlations and baseline regressions; test interactions (Height×Era, Height×Position, maybe Height×Era×Position); choose controls (minutes, games; age if available)                                               | Select primary model + backup model; run robustness checks (different metrics, thresholds); prep EDA Checkpoint submission polishing    |\n",
    "| **3/04**     | 6 PM            | Submit EDA Checkpoint (Due 3/04): clean plots, captions, and clear takeaways; confirm reproducibility (run-all); outline what modeling will prove beyond EDA                                                                                       | Begin full analysis write-up; generate final results tables (effect sizes + CIs); start final visualization polishing                   |\n",
    "| **3/08**     | 6 PM            | Main analysis + interpretation: finalize models; translate coefficients into plain language (e.g., “+2 inches → X change in 3PA/36 in 90s vs 2010s”); finalize figure set (by-era trend lines, coefficient plot)                                       | Draft Results + Discussion; document limitations (position labeling, pace/era confounds, survivorship); update README + methods details |\n",
    "| **3/11**     | 6 PM            | Full draft assembly: Methods/Results/Discussion integration; ensure claims match evidence; peer review for clarity + logic; confirm citations and formatting                                                                                           | Produce full project draft v1; polish visuals + captions; clean code and add comments; finalize conclusion                              |\n",
    "| **3/18**     | Before 11:59 PM | Submission checklist: final run-through of notebook(s), report, figures; verify file names and rubric requirements; confirm all group surveys done                                                                                                     | **Turn in Final Project & Group Project Surveys**                                                                                       |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
