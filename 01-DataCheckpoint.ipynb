{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
    "- Bob Barker:  Analysis, Software, Visualization\n",
    "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
    "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft\n",
    "\n",
    "Ja-Chan Lu: Updated research question, updated data after commentary on data section from project proposal.\n",
    "\n",
    "Tianlin Situ:\n",
    "\n",
    "Julian Luan:\n",
    "\n",
    "Tony Zhang:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates to Research Question:\n",
    "\n",
    "**Defining \"Eras\":**\n",
    "\n",
    "***Era 1 (1990-2000 seasons):*** Big-man dominated era where most points were scored within close distances to the basket as opposed to three-pointers (much further away from the basket).\n",
    "\n",
    "***Era 2 (2001-2013 seasons):*** League begins to shy away from traditional post-up and big-men due to the removal of the \"illegal defense\" which demanded players must be glued to the player they are defending at all times. With the abolishment of that rule, teams were allowed to play zone defense, which allowed players to occupy space on the court and prevent the other team from taking it instead of solely sticking to the player. In zone defenses, teams could theoretically just sufforcate the paint with their players and prevent anyone from making their way into the basket, taking away points from closer to the basket. To counter this, teams began to space out their players in the court, in attempts to work with the little space they have in the perimters to score from further away the basket. \n",
    "\n",
    "***Era 3 (2014-Present seasons):*** Teams analyzed shot attempts throughout their seasons and come to the realization that mid-range shots just are not as efficient as shooting 3-pointers. The chances of missing a mid-range and a three-pointer are the same, but three-pointers reward more points than a mid-range, meaning teams would much rather shoot 3's going forward. Teams now push all their players to attempt shooting 3-pointers, rather than mid-range jumpers, further increasing the volume of 3-pointers in the league.\n",
    "\n",
    "**Editting Reserach Question:**\n",
    "\n",
    "***New Research Question:*** How has the correlation between height and 3-point attempts changed across select NBA eras? Furthermore, we can then pursue another question: Is the increase in 3-point shooting volume only apply to taller positions (power forwards and centers) or does this affect all positions equally?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update to Data Proposal Section\n",
    "\n",
    "Actual Dataset Description:\n",
    "Our dataset will be from swar/nba_api python package, which is an open-source client that gives access to the official data API's on NBA.com. The package is widely used and reliable for accessing NBA data, as seen with their 3,400 GitHub stars. \n",
    "\n",
    "As mentioned earlier, the data comes directly from NBA.com's database that gives access to player statistics. For this project, we will utilize the playercareerstats endpoint which keeps records of the player information needed, starting from the 1990 NBA season up until the present day.\n",
    "\n",
    "For our analysis, we will sample 3-4 datasets (seasons) from each era defined above (1990-2000, 2001-2013, and 2014-present day. Under the assumption that each NBA season has around 500 players, we will have roughly 6000 total observations, where each observation contains a player's id, team_id, height, wingspan, position, points, three_point_attempts, games_played, and minutes_played.\n",
    "\n",
    "The data in the nba_api originates from official NBA box scores and logs, that were maintained by the league since the NBA was created. During 1990s-2000 era, the statistics were recorded by officials manually whereas 2001-present day, the data was logged and recorded by automated system. Whether it was automated or manually recorded, the data is still reliable and truthful as the NBA continues to verify recorded statistics both during and after games.\n",
    "\n",
    "Limitations to the data:\n",
    "1. Records of heights: Throughout all eras, players are to self-report height, and oftentimes may be rounded up/down, rather than 100% truthful.\n",
    "2. Position Labels: Although all players are given one specific position on paper, their roles on the court often change situationally, meaning no player is truly confined to one role throughout the whole season.\n",
    "3. In order to ensure accuracy, players who have less than 41 games_played or 10 minutes_played will not be considered in the dataset to prevent skewing the analysis as they will most likely have exceptionally high/low statistics that do not truly reflect their abilities as a player. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATED**\n",
    "| Meeting Date | Meeting Time    | Agenda / Discuss at Meeting                                                                                                                                                                                                                            | Next Steps                                                                                                                              |\n",
    "| ------------ | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **2/16**     | 6 PM            | Finalize scope: choose eras/seasons (e.g., 90s/00s/10s or specific season ranges); decide unit of analysis (player-season); decide primary metric (3PA/game vs 3PA/36 vs 3PA rate = 3PA/FGA); confirm position grouping plan (G/F/C or Guard/Wing/Big) | Assign roles (data pull, cleaning, EDA, modeling, writing); set repo structure; draft data dictionary + analysis plan outline           |\n",
    "| **2/18**     | 6 PM            | Data Checkpoint work session: pull data via `nba_api`; verify joins (player_id ↔ season stats ↔ height); check coverage across eras; start cleaning (height standardization, position parsing, era labeling)                                       | Submit Data Checkpoint; export raw dataset v1 + cleaned dataset v1; document endpoints + code workflow in README                        |\n",
    "| **2/23**     | 6 PM            | Data cleaning review: handle missing height/position; decide filters (min minutes/games); engineer variables (3PA/36, 3PA rate); sanity checks by era/position (counts, ranges)                                                                        | Lock cleaning rules; produce final analysis-ready dataset; write short “Data & Cleaning” methods paragraph                              |\n",
    "| **2/25**     | 6 PM            | EDA sprint: scatter/hex plots of height vs 3PA metrics; stratify by era and position; check whether correlation differs by group; identify outliers (very tall high-3PA) and decide treatment                                                          | Finalize EDA notebook + 3–5 key figures; draft EDA narrative answering “overall vs by era/position”                                     |\n",
    "| **3/01**     | 6 PM            | Modeling plan + first results: run correlations and baseline regressions; test interactions (Height×Era, Height×Position, maybe Height×Era×Position); choose controls (minutes, games; age if available)                                               | Select primary model + backup model; run robustness checks (different metrics, thresholds); prep EDA Checkpoint submission polishing    |\n",
    "| **3/04**     | 6 PM            | Submit EDA Checkpoint (Due 3/04): clean plots, captions, and clear takeaways; confirm reproducibility (run-all); outline what modeling will prove beyond EDA                                                                                       | Begin full analysis write-up; generate final results tables (effect sizes + CIs); start final visualization polishing                   |\n",
    "| **3/08**     | 6 PM            | Main analysis + interpretation: finalize models; translate coefficients into plain language (e.g., “+2 inches → X change in 3PA/36 in 90s vs 2010s”); finalize figure set (by-era trend lines, coefficient plot)                                       | Draft Results + Discussion; document limitations (position labeling, pace/era confounds, survivorship); update README + methods details |\n",
    "| **3/11**     | 6 PM            | Full draft assembly: Methods/Results/Discussion integration; ensure claims match evidence; peer review for clarity + logic; confirm citations and formatting                                                                                           | Produce full project draft v1; polish visuals + captions; clean code and add comments; finalize conclusion                              |\n",
    "| **3/18**     | Before 11:59 PM | Submission checklist: final run-through of notebook(s), report, figures; verify file names and rubric requirements; confirm all group surveys done                                                                                                     | **Turn in Final Project & Group Project Surveys**                                                                                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
